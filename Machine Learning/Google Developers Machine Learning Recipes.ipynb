{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1 - Hello World with ML\n",
    "\n",
    "[Tutorial 1 - Hello World with Machine Learning Recipes](https://www.youtube.com/watch?v=cKxRvEZd3Mw)\n",
    "\n",
    "Write Code that Can Tell the Difference Between an Apple and Orange\n",
    "\n",
    "\n",
    "## Debian Installation\n",
    "`pip install -U scikit-learn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test that you have the sklearn library installed.\n",
    "\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "\n",
    "Collect Training Data --> Train Classifier --> Make Predictions\n",
    "\n",
    "For each peice of fruit, we'll collect data.  This will be training data.\n",
    "\n",
    "Data is called \"Features\" and good data makes it easy to discriminate well.  The whole table is our training data.  The more data we have, the better we can discriminate.\n",
    "\n",
    "1.  We get our data.\n",
    "2.  We setup our decision tree or \"classifier\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "features = [[140,1], [130,1], [150,0], [170,0]]  # Weight in grams;  0 is bumpy, 1 is smooth.\n",
    "labels = [0,0,1,1] #0 is an apple, 1 is an orange.\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()  # Create the classifier.  This is now an empty box of rules.\n",
    "\n",
    "# Learning algorithm is the procedure that creates the rule.  \n",
    "\n",
    "clf = clf.fit(features, labels)\n",
    "\n",
    "print(clf.predict([[160,0]]))  # 150 grams, and bumpy\n",
    "print(clf.predict([[100,1]]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2 - Visualize a Decision Tree\n",
    "\n",
    "[Classifying flowers.](https://www.youtube.com/watch?v=tNa99PG8hR8&list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal&index=2) using a decision tree because . . . it's easy to see and visualize.\n",
    "\n",
    "1. Import Dataset\n",
    "2. Train a classified\n",
    "3. Predict a label for the new flower.\n",
    "4. Visualize the tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[0 1 2]\n",
      "[ 5.1  3.5  1.4  0.2] 0\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "\n",
    "iris = load_iris()\n",
    "test_idx = [0,50,100]\n",
    "\n",
    "# Training Data\n",
    "train_target = np.delete(iris.target, test_idx)\n",
    "train_data = np.delete(iris.data, test_idx, axis=0)\n",
    "\n",
    "#Testing Data \n",
    "test_target= iris.target[test_idx]\n",
    "test_data = iris.data[test_idx]\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(train_data, train_target)\n",
    "\n",
    "print(test_target)\n",
    "print(clf.predict(test_data))  # We give it the features, get back labels.  Predict labels for new data\n",
    "\n",
    "# Visualization\n",
    "\n",
    "''' \n",
    "from sklearn.externals.six import StringIO\n",
    "import pydot\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf,\n",
    "                    out_file=dot_data,\n",
    "                    feature_names=iris.feature_names,\n",
    "                    class_names=iris.target_names,\n",
    "                    filled=True,\n",
    "                    rounded=True,\n",
    "                    impurity=False)\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_pdf(\"iris.pdf\")\n",
    "'''\n",
    "print(test_data[0], test_target[0])\n",
    "print(iris.feature_names, iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Makes a Good Feature? - Machine Learning Recipes\n",
    "\n",
    "[Video](https://www.youtube.com/watch?v=N9fDIAflCMY&list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal&index=3) - we go over what makes a good feature.\n",
    "\n",
    "What makes a good feature?  A good feature in binary applications is a way to easily tell between two things.\n",
    "Dogs Example:  Height vs eye color.  One is useful and the other is not.  \n",
    "\n",
    "Thought Experiment:  If you were a machine trying to figure out what you were looking at, what other things would you want to know?  hair length, speed, weight.  Number of parameters is really an art.  If a feature doesn't correlate, it's useless and can hurt the outcome of the data.  \n",
    "\n",
    "Independent Features:  Want our features to be independent.  For example:  height in inches and height in cm are highly correlated, and might be double counted how important the features are.\n",
    "\n",
    "*Easy to Understand Features:*  The easier to understand, the easier to build a model on.  Simpler relationships are easier to learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADthJREFUeJzt3X+oZGd9x/H3pzG1pQpJmpuw3d10\nU9nSxFJXuQ2B9I9U2xpTcSM0JaHVRQJrIYEI9kfiP9qCYKGaIrSBtaZZwapLjWQpoW26plj/MHo3\nbmPiGtxqmlx32V3rr4iQssm3f8y5zbjO3jv3zszOnmffLxhmzjPPmfN9OHc/e+5zzpmbqkKS1K6f\nmncBkqTZMuglqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjXvZvAsAuPTSS2vbtm3z\nLkOSeuXgwYPfrqqFtfqdE0G/bds2lpaW5l2GJPVKkv8ep59TN5LUOINekhpn0EtS4wx6SWqcQS9J\njTPoJalxBr0kNc6gl6TGGfSS1Lhz4s5YaS3J/LZdNb9tS9PgEb0kNc6gl6TGGfSS1DiDXpIaZ9BL\nUuMMeklqnEEvSY1bM+iTbE3ySJLDSZ5McmfX/r4k30pyqHvcOLTO3UmOJHkqyRtnOQBJ0urGuWHq\nFPDuqnosySuBg0ke7t67p6r+arhzkquBW4BXA78A/FuSX66qF6ZZuCRpPGse0VfVsap6rHv9HHAY\n2LzKKjuBT1bV81X1TeAIcM00ipUkrd+65uiTbANeCzzaNd2R5PEk9yW5uGvbDDw7tNoyI/5jSLI7\nyVKSpZMnT667cEnSeMYO+iSvAD4NvKuqfgDcC7wK2AEcAz640nXE6j/xbSFVtaeqFqtqcWFhYd2F\nS5LGM1bQJ7mQQch/vKoeAKiq41X1QlW9CHyEl6ZnloGtQ6tvAY5Or2RJ0nqMc9VNgI8Ch6vqQ0Pt\nm4a6vRV4onu9H7glycuTXAlsB744vZIlSesxzlU31wFvA76S5FDX9h7g1iQ7GEzLPA28E6Cqnkyy\nD/gqgyt2bveKG0manzWDvqo+z+h594dWWef9wPsnqEuSNCXeGStJjTPoJalxBr0kNc6gl6TGGfSS\n1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mN\nM+glqXHj/M1Y6f9l1B+VlHRO84hekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiD\nXpIaZ9BLUuMMeklq3JpBn2RrkkeSHE7yZJI7u/ZLkjyc5Ovd88Vde5J8OMmRJI8ned2sByFJOrNx\njuhPAe+uqquAa4Hbk1wN3AUcqKrtwIFuGeBNwPbusRu4d+pVS5LGtmbQV9Wxqnqse/0ccBjYDOwE\n9nbd9gI3da93Ah+rgS8AFyXZNPXKpbMkmc9DmpZ1zdEn2Qa8FngUuLyqjsHgPwPgsq7bZuDZodWW\nuzZJ0hyMHfRJXgF8GnhXVf1gta4j2mrE5+1OspRk6eTJk+OWIUlap7GCPsmFDEL+41X1QNd8fGVK\npns+0bUvA1uHVt8CHD39M6tqT1UtVtXiwsLCRuuXJK1hnKtuAnwUOFxVHxp6az+wq3u9C3hwqP3t\n3dU31wLfX5nikSSdfeP8KcHrgLcBX0lyqGt7D/ABYF+S24BngJu79x4CbgSOAD8C3jHViiVJ67Jm\n0FfV5xk97w7whhH9C7h9wrokSVPinbGS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJek\nxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqc\nQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcWsGfZL7kpxI8sRQ\n2/uSfCvJoe5x49B7dyc5kuSpJG+cVeGSpPGMc0R/P3DDiPZ7qmpH93gIIMnVwC3Aq7t1/jbJBdMq\nVpK0fmsGfVV9DvjOmJ+3E/hkVT1fVd8EjgDXTFCfJGlCk8zR35Hk8W5q5+KubTPw7FCf5a5NkjQn\nGw36e4FXATuAY8AHu/aM6FujPiDJ7iRLSZZOnjy5wTIkSWvZUNBX1fGqeqGqXgQ+wkvTM8vA1qGu\nW4CjZ/iMPVW1WFWLCwsLGylDkjSGDQV9kk1Di28FVq7I2Q/ckuTlSa4EtgNfnKxESdIkXrZWhySf\nAK4HLk2yDLwXuD7JDgbTMk8D7wSoqieT7AO+CpwCbq+qF2ZTuiRpHKkaOYV+Vi0uLtbS0tK8y9AY\nMuosjGbiHPinqXNckoNVtbhWP++MlaTGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXO\noJekxhn0ktS4Nb/rRjrf1chv3z47W5amwSN6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1Dgvr1Qv\nzO8SR6n/PKKXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc4bpnoo3jskaR08opek\nxhn0ktQ4g16SGrdm0Ce5L8mJJE8MtV2S5OEkX++eL+7ak+TDSY4keTzJ62ZZvCRpbeMc0d8P3HBa\n213AgaraDhzolgHeBGzvHruBe6dTpiRpo9YM+qr6HPCd05p3Anu713uBm4baP1YDXwAuSrJpWsVK\nktZvo5dXXl5VxwCq6liSy7r2zcCzQ/2Wu7ZjGy9ROj/N6zLaqvlsV7Mz7ZOxo340R/7YJNmdZCnJ\n0smTJ6dchiRpxUaD/vjKlEz3fKJrXwa2DvXbAhwd9QFVtaeqFqtqcWFhYYNlSJLWstGg3w/s6l7v\nAh4can97d/XNtcD3V6Z4JEnzseYcfZJPANcDlyZZBt4LfADYl+Q24Bng5q77Q8CNwBHgR8A7ZlCz\nJGkd1gz6qrr1DG+9YUTfAm6ftChJ0vR4Z6wkNc6gl6TG+TXF0jmqRl6tfHa2rLZ4RC9JjTPoJalx\nBr0kNc6gl6TGeTJW6zK/E4SSNsojeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1Lj\nDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6g\nl6TGGfSS1LiXTbJykqeB54AXgFNVtZjkEuBTwDbgaeD3q+q7k5UpSdqoaRzR/2ZV7aiqxW75LuBA\nVW0HDnTLkqQ5mcXUzU5gb/d6L3DTDLYhSRrTpEFfwL8mOZhkd9d2eVUdA+ieL5twG5KkCUw0Rw9c\nV1VHk1wGPJzka+Ou2P3HsBvgiiuumLAMSdKZTHREX1VHu+cTwGeAa4DjSTYBdM8nzrDunqparKrF\nhYWFScqQJK1iw0Gf5OeSvHLlNfA7wBPAfmBX120X8OCkRUqSNm6SqZvLgc8kWfmcf6iqf07yJWBf\nktuAZ4CbJy9TkrRRGw76qvoG8JoR7f8DvGGSoiRJ0+OdsZLUOINekho36eWV57XB6QlJOrd5RC9J\njTPoJalxBr0kNc45ekk/Zp7nnqrmt+2WeUQvSY0z6CWpcQa9JDXOOXpJP6aY5w0iTtLPgkf0ktQ4\ng16SGmfQS1LjDHpJapxBL0mNM+glqXFeXtlD8738TVLfeEQvSY0z6CWpcQa9JDXOoJekxhn0ktQ4\ng16SGufllZLOHfP681aN/2krj+glqXEGvSQ1zqCXpMb1fo5+nn+xXpL6oPdBP09+54ykPpjZ1E2S\nG5I8leRIkrtmtR1J0upmckSf5ALgb4DfBpaBLyXZX1VfncX2JGki85wDPguXds7qiP4a4EhVfaOq\n/hf4JLBzRtuSJK1iVkG/GXh2aHm5a5MknWWzOhk76vegH/v9JMluYHe3+MMkT82olnFdCnx7PSv0\n4FTsusfUEy2OyzH1w/THNNm00S+O02lWQb8MbB1a3gIcHe5QVXuAPTPa/rolWaqqxXnXMU0tjgna\nHJdj6oe+jmlWUzdfArYnuTLJTwO3APtntC1J0ipmckRfVaeS3AH8C3ABcF9VPTmLbUmSVjezG6aq\n6iHgoVl9/gycM9NIU9TimKDNcTmmfujlmFKNfz2nJJ3v/FIzSWrceRn0Se5LciLJE0Nt70vyrSSH\nuseN86xxvZJsTfJIksNJnkxyZ9d+SZKHk3y9e7543rWOa5Ux9XZfJfmZJF9M8p/dmP68a78yyaPd\nfvpUdxFDL6wypvuTfHNoP+2Yd63rleSCJF9O8k/dci/303kZ9MD9wA0j2u+pqh3do0/nFwBOAe+u\nqquAa4Hbk1wN3AUcqKrtwIFuuS/ONCbo7756Hnh9Vb0G2AHckORa4C8ZjGk78F3gtjnWuF5nGhPA\nnwztp0PzK3HD7gQODy33cj+dl0FfVZ8DvjPvOqapqo5V1WPd6+cY/HBuZvDVE3u7bnuBm+ZT4fqt\nMqbeqoEfdosXdo8CXg/8Y9fet/10pjH1WpItwO8Cf9cth57up/My6FdxR5LHu6md3kxxnC7JNuC1\nwKPA5VV1DAbBCVw2v8o27rQxQY/3VTcdcAg4ATwM/Bfwvao61XXp3VeGnD6mqlrZT+/v9tM9SV4+\nxxI34q+BPwVe7JZ/np7uJ4P+JfcCr2Lwq+cx4IPzLWdjkrwC+DTwrqr6wbzrmYYRY+r1vqqqF6pq\nB4M7xq8BrhrV7exWNZnTx5TkV4G7gV8Bfh24BPizOZa4LkneDJyoqoPDzSO69mI/GfSdqjre/bC+\nCHyEwT/AXklyIYNA/HhVPdA1H0+yqXt/E4Mjrt4YNaYW9hVAVX0P+HcG5x8uSrJyX8tPfGVIXwyN\n6YZu6q2q6nng7+nXfroOeEuSpxl8++7rGRzh93I/GfSdlTDsvBV44kx9z0Xd/OFHgcNV9aGht/YD\nu7rXu4AHz3ZtG3WmMfV5XyVZSHJR9/pngd9icO7hEeD3um5920+jxvS1oQOMMJjL7s1+qqq7q2pL\nVW1j8BUun62qP6Cn++m8vGEqySeA6xl8E91x4L3d8g4Gv4o9DbxzZW67D5L8BvAfwFd4aU7xPQzm\ntPcBVwDPADdXVS9ORK8yplvp6b5K8msMTuJdwOBAa19V/UWSX2Jw5HgJ8GXgD7sj4XPeKmP6LLDA\nYMrjEPBHQydteyPJ9cAfV9Wb+7qfzsugl6TziVM3ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEG\nvSQ1zqCXpMb9H6zuGnfkMMYcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181da218080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "greyhounds = 500\n",
    "labs = 500\n",
    "\n",
    "grey_height = 28 + 4 * np.random.randn(greyhounds)  # Make the height +/- 4 inches\n",
    "lab_height = 24 + 4 * np.random.randn(labs)\n",
    "\n",
    "plt.hist([grey_height, lab_height], stacked=True, color=['r', 'b'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a Pipeline\n",
    "\n",
    "[Video](https://www.youtube.com/watch?v=84gqSbLcBFE&list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal&index=4)\n",
    "\n",
    "Imagine a spam classifier.  One approach is to partition our dataset into two groups:  one for training and one for testing to see how it works.\n",
    "\n",
    "In the below example, f(x) = y).  Features are x, and the labels are y.\n",
    "\n",
    "While there are many types of classifiers, at a high level they have the same interface.  In ML we don't write the algorithm, but we let the classifier write it from a model of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 2 0 0 1 1 0 0 2 1 1 2 1 2 2 2 1 0 1 1 1 0 1 1 0 1 0 1 2 0 0 1 0 0 2\n",
      " 0 2 1 2 1 1 0 1 2 2 2 1 2 0 0 0 1 2 0 1 2 2 1 0 1 0 1 0 1 0 1 2 0 2 1 2 2\n",
      " 2]\n",
      "0.906666666667\n",
      "[1 1 0 2 0 0 1 1 0 0 2 1 1 2 1 2 2 2 1 0 1 1 1 0 1 1 0 1 0 1 2 0 0 1 0 0 2\n",
      " 0 2 2 2 1 1 0 1 2 2 2 1 2 0 0 0 1 2 0 1 2 2 1 0 1 0 1 0 1 0 1 2 0 1 1 2 2\n",
      " 2]\n",
      "0.92\n"
     ]
    }
   ],
   "source": [
    "# import a dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data   # Feature\n",
    "y = iris.target # Label\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)  # Test size divides up the data (50% test and 50% train)\n",
    "\n",
    "from sklearn import tree\n",
    "my_classifier = tree.DecisionTreeClassifier()   # We're using a Decision Tree Classifier\n",
    "\n",
    "my_classifier.fit(X_train, y_train)             # We use fit with our training data to make the classifier\n",
    "\n",
    "predictions = my_classifier.predict(X_test)     # We use the classifier on the test data.\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))       # Prints the percent accuracy!\n",
    "\n",
    "##\n",
    "#  Now use a KNeighbors classfier instead of decision tree.\n",
    "##\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "my_classifier = KNeighborsClassifier()          # Now we'll use our KNeighbors classifier\n",
    "\n",
    "my_classifier.fit(X_train, y_train)             # We use fit with our training data to make the classifier\n",
    "predictions = my_classifier.predict(X_test)     # We use the classifier on the test data.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))       # Prints the percent accuracy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write our First Classifier\n",
    "\n",
    "[Video](https://www.youtube.com/watch?v=AoeEHqVSNOw&index=5&list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal)\n",
    "\n",
    "Previous we imported the library, but in this we'll write our own classifier. \n",
    "\n",
    "First we make a class to cover this. Measure the euclidean distance between points. Euclidean distance measure around as many dimensions as we can add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.973333333333\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Measure the euclidean distance. \n",
    "def euc(a,b):\n",
    "    return distance.euclidean(a,b)\n",
    "\n",
    "# This classifier will work based on the euclidean distance, and try to find a short distance.\n",
    "class ScrappyKNN():\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for row in X_test:\n",
    "            label = self.closest(row)  # Implement the closesest neighbor.\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    def closest(self, row):\n",
    "        best_dist = euc(row, self.X_train[0])  # Calculate the first test point.\n",
    "        best_index = 0\n",
    "        for i in range(1, len(self.X_train)):  # ITerate over all the other training points and update when we find somethign closer.\n",
    "            dist = euc(row, self.X_train[i])\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                best_index = i\n",
    "        return self.y_train[best_index]\n",
    "            \n",
    "# import a dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data   # Feature\n",
    "y = iris.target # Label\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)  # Test size divides up the data (50% test and 50% train)\n",
    "\n",
    "##\n",
    "#  Now use a KNeighbors classfier instead of decision tree.\n",
    "##\n",
    "\n",
    "#  from sklearn.neighbors import KNeighborsClassifier\n",
    "my_classifier = ScrappyKNN()\n",
    "\n",
    "# my_classifier = KNeighborsClassifier()          # Now we'll use our KNeighbors classifier\n",
    "\n",
    "my_classifier.fit(X_train, y_train)             # We use fit with our training data to make the classifier\n",
    "predictions = my_classifier.predict(X_test)     # We use the classifier on the test data.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))       # Prints the percent accuracy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
